{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# At this point the test and training data have already been split and processed.\n",
    "# Running the Random Forest over various parameters and training and validation sets before using the best combination \n",
    "# For the test data\n",
    "# Parameters : Part 1\n",
    "#              X_train = Training data without validation data\n",
    "#              X_valid = Validation data\n",
    "#\n",
    "#              Part 3\n",
    "#              X_train = Training data + validation data\n",
    "#              X_test = Test data\n",
    "\n",
    "### Part 1 : Hyperparameter tuning\n",
    "\n",
    "max_depth = np.arange(30) + 1\n",
    "min_samples_leaf = np.arange(50) + 1\n",
    "n_estimators = np.arange(50) + 1\n",
    "train_depth_acc = []\n",
    "valid_depth_acc = []\n",
    "train_leaf_acc = []\n",
    "valid_leaf_acc = []\n",
    "train_est_acc = []\n",
    "valid_est_acc = []\n",
    "\n",
    "for depth in max_depth:\n",
    "    clf = DecisionTreeClassifier(max_depth=depth)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_depth_acc.append(clf.score(X_train, y_train))\n",
    "    valid_depth_acc.append(clf.score(X_valid, y_valid))\n",
    "\n",
    "print(valid_depth_acc[valid_depth_acc.index(max(valid_depth_acc))])\n",
    "print(max_depth[valid_depth_acc.index(max(valid_depth_acc))])\n",
    "\n",
    "plt.plot(max_depth, train_depth_acc, color='blue', label='training')\n",
    "plt.plot(max_depth, valid_depth_acc, color='orange', label='validation')\n",
    "plt.title('Maximum depth vs Accuracy')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for min_leaf in min_samples_leaf:\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=min_leaf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_leaf_acc.append(clf.score(X_train, y_train))\n",
    "    valid_leaf_acc.append(clf.score(X_valid, y_valid))\n",
    "\n",
    "print(valid_leaf_acc[valid_leaf_acc.index(max(valid_leaf_acc))])\n",
    "print(min_samples_leaf[valid_leaf_acc.index(max(valid_leaf_acc))])\n",
    "\n",
    "plt.plot(min_samples_leaf, train_leaf_acc, color='blue', label='training')\n",
    "plt.plot(min_samples_leaf, valid_leaf_acc, color='orange', label='validation')\n",
    "plt.title('Minimum samples per leaf vs Accuracy')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for est in n_estimators:\n",
    "    clf = RandomForestClassifier(n_estimators=est)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_est_acc.append(clf.score(X_train, y_train))\n",
    "    valid_est_acc.append(clf.score(X_valid, y_valid))\n",
    "\n",
    "plt.plot(n_estimators, train_est_acc, color='blue', label='training')\n",
    "plt.plot(n_estimators, valid_est_acc, color='orange', label='validation')\n",
    "plt.title('Number of estimators vs Accuracy')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### Put the best parameters here, currently at default values to test code\n",
    "### Change after hyperparameter tuning\n",
    "\n",
    "best_n = 10\n",
    "best_depth = None \n",
    "best_leaf = 1\n",
    "\n",
    "### Part 3 : Testing data\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=best_n, max_depth=best_depth, min_samples_leaf=best_leaf)\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Random Forest Training Accuracy = \" + str(train_acc))\n",
    "print(\"Random Forest Testing Accuracy = \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Insert here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOXICITY CLASSIFICATION WITH REDUCED\n",
    "UNINTENDED BIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Members\n",
    "1. Agnes Sharan Sahaya Raj Helan - asr647\n",
    "2. Jairam Venkitachalam - jv1589\n",
    "3. Srishti Bhargava - sb7261 (Member responsible for uploading submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION <br>\n",
    "Discussion on online platforms can be difficult. A constant fear of abuse and harassment impedes\n",
    "people from expressing their opinions, which in turn results in platforms being unable to facilitate\n",
    "and foster an environment for stimulating conversations. This form of cyber bullying has adverse\n",
    "effects on the psychology of participants as well. Such a scenario does not bode well, neither for the\n",
    "participants, nor for the platform facilitating the conversations. There have been various attempts\n",
    "at building models that can understand and classify comments into different classes depending on\n",
    "their toxicity, with an aim of making online discussions more productive and respectful.\n",
    "However, these models often have had a history of associating even the comments mean to foster\n",
    "fruitful discussions on ostracised communities with toxicity due to word associations and hence\n",
    "lead to erroneous classification of these comments. We aim to build a machine learning model,\n",
    "that makes it possible for toxic comments to be identified and also reduces the misclassification of\n",
    "comments due to unintended bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology : Algorithm and Models <br>\n",
    "Three modelling algorithms were used to solve the problem. They are: <br>\n",
    "1. Logistic Regression <br>\n",
    "This first model was chosen for its range of use in classification problems and in predicting the probabilities of data points belonging to different classes. By nature of the dataset set, the target value in the dataset is continuous probability and the goal is to predict the nature of class of the comment using probabilistic analysis and thresholding the value obtained at 0.5 to predict toxic and non-toxic comments with $\\geq 0.5$ being toxic. <br>\n",
    "\t\n",
    "2. Random Forests <br>\n",
    "Decision trees attempt to predict the class of the data point using a restricted subset of the features of the model. Due to the large representative models of the word embeddings/ document term matrices used in the data modelling, such a restrictive choice will prevent overfitting while still being capable of filtering the most prevalent features.\\\\\\\\\n",
    "However one qualm of decision trees is their tendency to over fit on their training data and in order to overcome this an ensemble model that harnesses the advantages of decision trees while still accounting for prevention of overfitting was required, which was then chosen to be Random Forests. These build a many decision trees using the training data and upon testing on thus built forest, output the mode of all output classes predicted by the trees. <br>\n",
    "\n",
    "3. Gradient Boosted Machines <br>\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES <br>\n",
    "[1] https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview/evaluation <br>\n",
    "[2] https://perspectiveapi.com/#/home <br>\n",
    "[3] https://twitter.com/jessamyn/status/900867154412699649 <br>\n",
    "[4] https://arxiv.org/abs/1906.08237 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments:\n",
    "\n",
    "    We first begin by training our baseline model. Following is the code generating the Machine Learning Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "import lightgbm as lgb\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import sklearn\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"\n",
    "    We simply read the dataset and return a Pandas Dataframe\n",
    "    \"\"\"\n",
    "    train=pd.read_csv(\"train.csv\")\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(data):\n",
    "    \"\"\"\n",
    "    We aim to remove special characters and punctuations from our text field within this function\n",
    "    \"\"\"\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "    \n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tfidf():\n",
    "    \"\"\"\n",
    "    The function preprocesses and generates representation for TF-IDF representation.\n",
    "    It returns X_train, y_train which are the dataset along with the labels for training, \n",
    "    and X_test, y_test which are the dataset along with the labels for testing.\n",
    "    \"\"\"\n",
    "    data = read_data()\n",
    "\n",
    "    train=data[:1500000]\n",
    "    test=data[1500000:]\n",
    "    print(\"Beginning Preprocessing\")\n",
    "    word_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='word',\n",
    "    token_pattern=r'\\w{1,}', stop_words='english', ngram_range=(1, 1), max_features=10000)\n",
    "    train_intermediate=parse(train)\n",
    "    print(\"Beginning Fitting the word\")\n",
    "    word_vectorizer.fit(train_intermediate)\n",
    "\n",
    "    X_train=word_vectorizer.transform(train['comment_text'])\n",
    "    X_test=word_vectorizer.transform(test['comment_text'])\n",
    "    y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "    y_test = np.where(test['target'] >= 0.5, 1, 0)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bow():\n",
    "    \"\"\"\n",
    "    The function preprocesses and generates representation for Bag of Words representation.\n",
    "    It returns X_train, y_train which are the dataset along with the labels for training, \n",
    "    and X_test, y_test which are the dataset along with the labels for testing.\n",
    "    \"\"\"\n",
    "    data = read_data()\n",
    "\n",
    "    train=data[:1500000]\n",
    "    test=data[1500000:]\n",
    "    print(\"Beginning Preprocessing\")\n",
    "    count_vectorizer = CountVectorizer(strip_accents='unicode', stop_words=stop_words.ENGLISH_STOP_WORDS,\n",
    "                                 analyzer='word', ngram_range=(1, 1), token_pattern=r'\\w{1,}')\n",
    "    train_intermediate=parse(train)\n",
    "    print(\"Beginning Fitting the word\")\n",
    "    count_vectorizer.fit(train_intermediate)\n",
    "\n",
    "    X_train=count_vectorizer.transform(train['comment_text'])\n",
    "    X_test=count_vectorizer.transform(test['comment_text'])\n",
    "    y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "    y_test = np.where(test['target'] >= 0.5, 1, 0)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(x_axis, y_axis1, yaxis2, xlabel, ylabel, title):\n",
    "    \"\"\"\n",
    "    Skeleton code for plotting graphs\n",
    "    \"\"\"\n",
    "    plt.plot(x_axis, y_axis1, color='blue', label='training')\n",
    "    plt.plot(x_axis, yaxis2, color='orange', label='validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_graph_single_val(x_axis, y_axis1, xlabel, ylabel, title):\n",
    "    \"\"\"\n",
    "    Skeleton code for plotting graphs\n",
    "    \"\"\"\n",
    "    plt.plot(x_axis, y_axis1, color='blue', label='training')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function performs Logistic Regression and requires as parameters the training and testing dataset\n",
    "    \"\"\"\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_train = lr.predict(X_train)\n",
    "    y_pred_test = lr.predict(X_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    print(accuracy_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    print(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation involves fiddling with parameters such as Max-Depth, Min-Sample Leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Contains definition for a Decision Tree classifier\n",
    "    \"\"\"\n",
    "\n",
    "    max_depth = np.arange(30) + 1\n",
    "    min_samples_leaf = np.arange(50) + 1\n",
    "    n_estimators = np.arange(50) + 1\n",
    "    train_depth_acc = []\n",
    "    valid_depth_acc = []\n",
    "    train_leaf_acc = []\n",
    "    valid_leaf_acc = []\n",
    "    train_est_acc = []\n",
    "    valid_est_acc = []\n",
    "\n",
    "    for depth in max_depth:\n",
    "        clf = DecisionTreeClassifier(max_depth=depth)\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_depth_acc.append(clf.score(X_train, y_train))\n",
    "        valid_depth_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    print(valid_depth_acc[valid_depth_acc.index(max(valid_depth_acc))])\n",
    "    print(max_depth[valid_depth_acc.index(max(valid_depth_acc))])\n",
    "    \n",
    "    plot_graph(max_depth, train_depth_acc, valid_depth_acc, 'max_depth', 'accuracy', 'Maximum depth vs Accuracy')\n",
    "\n",
    "    for min_leaf in min_samples_leaf:\n",
    "        clf = DecisionTreeClassifier(min_samples_leaf=min_leaf)\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_leaf_acc.append(clf.score(X_train, y_train))\n",
    "        valid_leaf_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(valid_leaf_acc[valid_leaf_acc.index(max(valid_leaf_acc))]))\n",
    "\n",
    "    plot_graph(min_samples_leaf, train_leaf_acc, valid_leaf_acc, 'min_leaf', 'accuracy', 'Minimum samples per leaf vs Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest\n",
    "    \"\"\"\n",
    "    max_depth = np.arange(30) + 1\n",
    "    min_samples_leaf = np.arange(50) + 1\n",
    "    n_estimators = np.arange(50) + 1\n",
    "    train_depth_acc = []\n",
    "    valid_depth_acc = []\n",
    "    train_leaf_acc = []\n",
    "    valid_leaf_acc = []\n",
    "    train_est_acc = []\n",
    "    valid_est_acc = []\n",
    "\n",
    "    for est in n_estimators:\n",
    "        clf = RandomForestClassifier(n_estimators=est, bootstrap=False, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_est_acc.append(clf.score(X_train, y_train))\n",
    "        valid_est_acc.append(clf.score(X_val, y_val))\n",
    "\n",
    "    plot_graph(n_estimators, train_est_acc, valid_est_acc, 'n_estimators', 'accuracy', 'Number of estimators vs Accuracy')\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_leaf=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    test_acc = clf.score(X_test, y_test)\n",
    "    print(\"Random Forest Training Accuracy with default parameters: \".format(str(train_acc)))\n",
    "    print(\"Random Forest Testing Accuracy with default parameters: \".format(str(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_model(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=80745)  # If scipy version>0.19, add shuffle=True\n",
    "    \n",
    "    lgb_train_data = lgb.Dataset(X_train, y_train)\n",
    "    lgb_validation_data = lgb.Dataset(X_validation, y_validation, reference=lgb_train_data)\n",
    "    \n",
    "    num_leaves = [i for i in range(2, 62)]\n",
    "    \n",
    "    min_trees=150\n",
    "    min_leaves = 70\n",
    "    min_train=1000\n",
    "    \n",
    "    training_acc = []\n",
    "\n",
    "    # Plotting graph for finding best leaves given the best num_trees:\n",
    "    for leaf in num_leaves:\n",
    "        lgb_params = {\n",
    "            \"objective\": \"binary\",\n",
    "            'metric': {'binary'},\n",
    "            'num_leaves': leaf,\n",
    "            'num_trees': 150,\n",
    "        }\n",
    "\n",
    "        model = lgb.train(params=lgb_params, train_set=lgb_train_data, valid_sets=[lgb_validation_data])\n",
    "        y_pred_train = model.predict(X_train)\n",
    "\n",
    "        accuracy_train = accuracy_score(y_train, y_pred_train.round())\n",
    "        if(accuracy_train<min_train):\n",
    "            min_train = accuracy_train\n",
    "            min_leaves = leaf\n",
    "        training_acc.append(accuracy_train)\n",
    "\n",
    "    # Training with the best parameters:\n",
    "    lgb_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        'metric': {'binary'},\n",
    "        'num_leaves': min_leaves,\n",
    "        'num_trees': min_trees,\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(params=lgb_params, train_set=lgb_train_data, valid_sets=[lgb_validation_data])\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train.round())\n",
    "    print(accuracy_train)\n",
    "\n",
    "    y_pred_val = model.predict(X_test)\n",
    "    accuracy_val=accuracy_score(y_test, y_pred_val.round())\n",
    "    print(accuracy_val)\n",
    "    plot_graph(num_leaves, training_acc, 'Num Leaves', 'Accuracy', 'Number of Leaves vs Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = preprocess_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
